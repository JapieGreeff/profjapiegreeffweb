				<!-- Main -->
					<div id="main">

						<!-- One -->
                            <div class="container">
                                <header class="major">
                                    <h2>The Challenge System</h2>
                                </header>				
                            </div>

                            <section>
								<div class="container">
									<p>As part of my doctoral studies, I developed a gamification based system that comprised of a number of gamefully designed components. Here I describe one of the gamified interventions that formed part of the project, the Challenge System.</p>
                                    <p>
                                        The Challenge System was built to supplement the work being done in the Tender Game and allow for students to demonstrate skills to study leaders before allocation of projects, while also allowing for a bank of challenges to be built up that could allow students to upskill in different areas should they need to during the course of the academic year. 
                                        The system had a low uptake in the initial trial, but in the second field trial when paired with the Tender Game showed a lot more promise. The system itself was not very complex in comparison to the other components in the overall gamification system, but did go to show that for systems like these to work most effectively, there is a need for a critical mass of content. 
                                    </p>
								</div>
							</section>
                             
                            <section>
								<div class="container">
                                    <h4>Overhead Design Summary of the Project</h4>
                                    <a href="#" class="image"><img src="../../../../images/OverheadDesignSummary.png" alt="" style="width : 100%;" /></a>							
									<p>The Challenge System was part of the overall project that comprised of a number of interventions at different times in the academic year during students' capstone project work.</p>
								</div>
							</section>

                            <section>
								<div class="container">
                                    <h4>Positioning in the Academic Year</h4>
                                    <a href="#" class="image"><img src="../../../../images/CourseStructureAndDeadlines.png" alt="" style="width : 100%;" /></a>							
									<p>The system was used before the allocations of projects, as well as through the course of the year for students to upskill in the areas they needed knowledge in. </p>
								</div>
							</section>

                            <section>
								<div class="container">
                                    <h4>Theoretical Foundations</h4>
									<p>The design of the Challenge System was heavily influenced by a number of theories and approaches:</p>
                                    <ul>
                                        <li>Superbetter Approach to Resilience: My primary inspiration for the Challenge System was the Superbetter framework put forward by Jane McGonigal. Superbetter was originally conceived to help individuals overcome significant life challenges, such as recovering from a concussion, by building mental, physical, emotional, and social resilience through gameful mechanics. I saw a parallel between these life challenges and the daunting big final goal of a capstone project.</li>
                                        <li>Self-Determination Theory (SDT): To ensure the system actually motivated students, I drew heavily from the work of Deci and Ryan on Self-Determination Theory. SDT posits that intrinsic motivation is fostered when three basic psychological needs are met: Autonomy, Competence (or Mastery), and Relatedness (or Community). My Challenge System addressed the first need by being entirely voluntary. I wanted students to feel that their choices mattered (Autonomy) and that by completing challenges, they were demonstrating their growth (Mastery). Furthermore, by making these achievements visible through points and badges which could be shared, I aimed to foster a sense of being part of a larger student community.</li>
                                        <li>Authentic Learning: Finally, I incorporated principles of Authentic Learning, as described by Abbott and Rule. This approach suggests that students are more motivated when they see that their learning reflects practical, real-life contexts. We designed the challenges to involve problems rooted in the real world and to empower learners through activities that were not done purely for a grade. By completing these tasks, students were building a portfolio of work that demonstrated their practical abilities to prospective study leaders before project allocation began, and could build on this through the course of the year. </li>
                                    </ul>
								</div>
							</section>

                            <section>
								<div class="container">
                                    <h4>Evolution of the Challenge System</h4>
									<p>The Challenge System went through three distinct design versions during the second to fourth prototype of the overall gamification system, each reflecting a shift in my understanding of user needs and system adoption.:</p>
                                        <ol>
                                            <li>Version 1: There was a focus on using the system for points scoring and vetting which influenced the Tender Game, and study leaders submitted challenges to the coordinator for vetting and points were assigned based on completion time.</li>
                                            <li>Version 2: The focus was on tie-breaking in tenders and the location based games, but the challenges were still submitted in the same way to the Course Coordinator, creating a lot of overhead.</li>
                                            <li>Version 3: The focus shifted to autonomy and badges for achievement, points were removed to eliminate benchmarking issues, and focus shifted to giving study leaders full autonomy to approve logs of students completing challenges and setting challenges without needing approval. </li>
                                        </ol>
                                    <p>In the initial version, I was overly concerned with ensuring that points were benchmarked correctly across different challenges. This required study leaders to submit their challenges to the coordinator for approval, which I realised later was a significant friction point and caused a lot of overhead. By the third iteration, I had removed the points system entirely, allowing study leaders to set challenges and approve logs independently, which significantly improved adoption.</p>
								</div>
							</section>

                            <section>
								<div class="container">
                                    <h4>Badges and Quests</h4>
                                    <a href="#" class="image"><img src="../../../../images/lbgQuestsCombined.png" alt="" style="width : 100%;" /></a>							
									<p>The Challenge System used the same badges and quests as the location based games as these were the themes that were represented by all of the projects done by all students, supervised by all study leaders.</p>
								</div>
							</section>                            

                            <section>
								<div class="container">
									<div class="features">							
                                        <article>
                                        <h4>Creating Challenges</h4>
										<a href="#" class="image"><img src="../../../../images/studyAreaChallenge.png" alt="" /></a>	
											<div class="inner">
												<p>
													For the final version, study leaders could just create their own challenges where they just needed to indicate that they were setting the challenge, a description of the challenge which could contain links to tutorials for example, and a description of what needed to be done to prove the challenge was completed successfully. My goal was to create a seamless interaction loop that mirrored the "questing" experience found in modern games. The resulting "Challenge Cycle" was designed to be as non-intrusive as possible while still providing the necessary feedback and rewards. The cycle was as follows:
												</p>
                                                <ol>
                                                    <li>Creation: A study leader identifies a specific skill they want to see in their prospective students (e.g., Canny edge detection) and fills out an online form indicating the study area, description, and "How to Beat this Challenge" instructions</li>
                                                    <li>Discovery: Students browse available challenges related to their interests on their dashboard. These challenges are tied to specific "Study Areas" such as Computer Vision, IoT, or Power Systems</li>
                                                    <li>Performance: The student performs the indicated task. There were no time limits, reducing pressure and allowing for self-directed learning</li>
                                                    <li>Submission: Once the task is complete, the student demonstrates the results to the study leader, often via email or physical proof, and marks the challenge as "complete" in the system</li>
                                                    <li>Approval: The study leader receives a notification of the completion log and, after reviewing the proof, either approves or declines the entry. Approved challenges result in points (in early versions) or badges (in later versions) appearing on the student's profile.</li>
                                                </ol>
											</div>
										</article>
									</div>
								</div>
							</section>                            

                            <section>
								<div class="container">
                                    <h4>Student Interactions</h4>
                                    <a href="#" class="image"><img src="../../../../images/challengeCompletion.png" alt="" style="width : 100%;" /></a>							
									<p>Students could view all challenges that were available for each of the study areas, and all they needed to do was mark that they had completed the challenge to create a log for the studyleader, and then to indicate how they would demonstrate the proof of completion. Study leaders got notifications of completion logs, and could then check the proof and mark whether they accepted or rejected the student's work.</p>
                                    <a href="#" class="image"><img src="../../../../images/challengeLog.png" alt="" style="width : 100%;" /></a>							
								</div>
							</section>  

                            <section>
								<div class="container">
                                    <h4>Exploring ECSA Exit-Level Outcomes</h4>
									<p>Part of the requirements put forth by the Engineering Council of South Africa on Engineering Degrees is that they need to meet dedicated Exit-Level Outcomes that describe work that students need to master in order to become candidate engineers. We explored these concepts through the overall project, but in the Challenge Based System this was particularly relevant, as some of the Exit-Level Outcomes mapped very well to this sort of intervention. The specific Exit-Level Outcomes we explored were:</p>
                                        <ul>
                                            <li>ELO 4 - Investications, Experiments and Data Analysis: Tasks requiring students to research frequencies or implement simulations were presented as challenges that fit this outcome.</li>
                                            <li>ELO 5 - Engineering Methods, Skills, and Tools:  Many challenges focussed on specific tools like Unity, Octave, or Programming Languages to solve problems like Canny edge detection.</li>
                                            <li>ELO 6 - Professional and Technical Communication: Writing-based challenges or the act of presenting proof to study leaders fit this outcome.</li>
                                            <li>ELO 9 - Independent Learning Ability: The voluntary and independent nature of the challenges encouraged self-directed skill acquisition which aligned well with this outcome.</li>
                                        </ul>
								</div>
							</section>

                            <section>
								<div class="container">
                                    <h4>Field Trial 2018</h4>
									<p>With the final version run in 2018 these were the challenges that were presented to students:</p>
                                        <ul>
                                            <li>Create a small smart phone app</li>
                                            <li>Research the frequencies Power Line Communication Systems operate on</li>
                                            <li>Describe your favourite projects from 2017 in a short report</li>
                                            <li>Generate a 1kHz tone</li>
                                            <li>Heat a steel wire using induction heating</li>
                                            <li>Create a simple rolling ball game in Unity</li>
                                            <li>Implement canny edge detection</li>
                                            <li>Create a system diagram of an automatic gate motor system</li>
                                            <li>Research the frequencies that Visual Light Communication Systems operate on</li>
                                            <li>Create a small Octave physics simulation</li>
                                        </ul>
                                    <p>With these challenges set at the start of the year, during the application phase of the Tender game, there were 16 completions logged by students, with the most popular project being the implementation of Canny edge detection which was completed by four students. There were many students that indicated that they were underway with challenges, but only 16 successful completions. Considering the voluntary nature of the challenges howver, we found similar completion rates to what one would expect from a MOOC.</p>
								</div>
							</section>

                            <section>
								<div class="container">
                                    <h4>Reflective Insights</h4>
									<p>
                                        During my project I reflected deeply on the Challenge System's implementation. I must admit that the final results, although in line with what was experienced with MOOCs, were disappointing to me. Based on my review of the literature, I had initially believed that this system would have the strongest impact on actual learning, as it focused on content gamification rather than the structural gamification of the other systems in the project. There were also some points that emerged based on reflection and discussion with users in the feedback sessions:
                                        <ul>
                                            <li>There was a motivation mismatch in the implementation of the system. I realised that there was a fundamental difference in how students engaged with different types of gamification. Structural gamification, like the Tender Game, achieved high adoption because it was tied to a mandatory process, ie. getting a project assigned. Content gamification, however, requires a higher level of internal motivation. I noted that students might not have seen the value in the challenges compared to the effort required, or they may have been intimidated by the prospect of failure, which we did not track or reward.</li>
                                            <li>Another key reflection was on the toll on study leaders' time. For such a system to be successful, it requires active participation from faculty members to post challenges and approve logs. Many study leaders were already overburdened and were hesitant to add another task to their workload. This led me to suggest that in future iterations, the creation of challenges should perhaps move to the students themselves. Allowing students to propose and track their own challenges would foster self-directed learning and self-assessment, which are crucial skills for an engineer.</li>
                                            <li>Perhaps my most significant reflection on the future of the Challenge System was the proposal to move away from a custom-built solution and instead leverage an open-source platform like Habitica. Habitica is a "life gamification" system that uses a pixel-art avatar and RPG mechanics to track daily habits and to-do lists. It allows for:
                                                <ul>
                                                    <li>Greater Visual Engagement: Using an avatar that gains equipment and levels provides a more constant sense of progress than abstract badges.</li>
                                                    <li>Detailed Tracking: Habitica allows for "checklists" within tasks, which would let us track progress and attempts, not just final success.</li>
                                                    <li>Community Ownership: Challenges in Habitica can be created by the community. By setting up a private "Guild" for our students, we could allow them to create challenges for each other, increasing their sense of ownership and community.</li>
                                                </ul>
                                            </li>
                                        </ul>
                                        Although the Challenge System did not achieve the viral adoption I had hoped for in my "rose-tinted" early vision, it provided invaluable data on the realities of educational gamification. I successfully proved that an agile, iterative approach to system design is necessary when dealing with complex human systems like a university department. I also learned that "toning down" gamification elements such as removing points might have been a mistake in the later iterations. In my final prototype evaluations, students expressed that they enjoyed the gameful approach and felt it helped their courses, but they were neutral on whether it diluted the academic content. I concluded that while the custom challenge system worked as designed, its future lies in more heavily gamified, community-driven platforms that reduce the burden on faculty while maximizing the autonomy and self-directed learning of the students. 
                                    </p>                                    
								</div>
							</section>
                    
                        </div>